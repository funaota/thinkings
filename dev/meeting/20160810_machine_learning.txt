
# AI/機械学習の今を探る

インフラがあればみんなAI作れる
そういうインフラを造るというのがグローバルの流れ

データサイエンスにはデータがまず大事

作業の流れ
・データ
・ER図
・必要データ選定
・データ抽出
・機械学習

機械学習は
・前処理で特徴量をどう出していくのか
・どのモデルで処理するのか
が難しい

ある程度のレベルのものはコモディティ化してきた
リクルートではコードが書けないような人でももう機械学習してる

教師データにもシグナルとしての強さがある

・SPIとレジュメで合否をもう判断している会社がある
・名札で誰と誰が話しているのかがわかり、もっとこの人と話したほうが生産性があがるみたいなサジェストができる
・うつ病への復職支援みたいな部分も数週間で当てられるようになった

機械学習やったもん勝ち

___________________________________________


# セレンディピティと機械学習

なぜ機械学習を学ぶのか？

情報推薦
	Matrix Factorization
	Probablilstic Matrix Factorization
	グラフィカルモデル
		因果関係を明示的に表現できる

	弱点
		似たようなアイテムばっかり推薦される（推薦の墓場）

その弱点を解決するためにセレンディピティが出てきた
セレンディピティを実現するためには、まず妄想で仮説を立てて、モデルを作り、実証していくしかない

評価されたデータだけでなく
コンテンツのデータ（潜在ベクトル）なども考慮していく
	Multi view clustering


___________________________________________

# 多様化された時代を制するためのAI・機械学習

ロングテール現象
テールの部分が売り上げの多くを占める

インターネットによって多様化、個別化されている社会では人力でそれに対応するのはとても難しい

EC業界でのAIはいかにロングテールを制するかが肝になっている

ECでもNLP結構大事

ブートストラップ法

質の高いレビュー情報のみの抜粋などにも使われている（評判解析）
流通総額が150億上がった

今の時代は、人間の仮説による企画が外れるようになってきた
LDAを使って、機械が企画していく
UCBアルゴリズム（売り上げアップ）
バンデットアルゴリズム

需要予測
初期は人力でのフィッティングが必要だったが、今ではデータドリブンでフィッティングすることができている

楽天ではクーポンを用いて、価格の個別化も行っている

グラフDB

AIは本質的に今あるビジネスプロセスを大幅に変えて、価値を出していく
じゃあ、何を身につけていくのか
どういうキャリアデベロップメントを狙っていくのか
結果的には、すべての職を奪っていく

人は今までにない枠組みを作っていく、
動かしてくことが人間が行なうべきクリエイティブな行為
秋元康すごい（AKBのCDの売上予想できなかった）


___________________________________


# 機械学習の学び方

やってみる
→ 数学はやれ（線形代数、微分・積分、確率・統計）

数学をやってみるより、たくさんデータ集めたほうがいいよねっていう考え方もある

業界の中でいろいろ流派があるから、俯瞰してみて自分が合う流派を探していくのが大事

ライブラリが鬼のように充実してて、数行で記述が終わるレベル

わからなくてもできちゃう
人工知能のライブラリを作るのなら、数学が必要
知ってたほうが得

1日に100本くらい論文が出るので、
ユーザーサイドならあまり読まなくてもいい

kaggleとかを使うと動的に学んでいくことができる

自分の師匠のような人を見つけていくと、
scikit-learn（定石的ないい感じの場所）（機械学習分野のみ）

AWSはGPUが唯一使えるから、deep learningがやりたい人にはいい
data robot
最初はローカルで動かしました。
deep learningは結構重たかった

目的を考える
目的から降ろして考えることは、機械学習にはできない
ここ考えるのは大事

deep learningなら
ケラス、tensor flow、

シグナルを色々考える（何がキーになるのか）
何が結果に対する大事な要素なのか考えて、データを集めてブッコメバだいじょうぶ

本番のサービスとは独立にテストしたりできる
試すこと自体はノーリスク

予測したいことがあれば、データさえあればだいたい結果は出る
いかにローコストで精度を出していくサイクルを作るのが大事
あと、データ量があれば大抵平気

しかし、どれくらい難しい問題を解こうとしているのかによる

コストと、デリバリーは見積もれる
精度はやってみないとわからない

機械学習系では、データの前処理が７、８割の作業量になるけどディープラーニングはその作業がない方が精度が出たりする
その精度が出るケースは、データが大量にあるケースのみ（画像とか）

結果が出たものをもう一回決定機にかけるとブラックボックス間ちょっと減る
オーバーフィッティング（使える評価データなのか）
評価戦略の立て方が結構大事

学習させた後には戻せないから、都度都度バックアップを取っておく
なんでうまくいったのかとかは、統計分析とかなどでちゃんとデータを見るのが大事

PFI
機械学習は苦しい（業務利用のための資料）